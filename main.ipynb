{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-01T06:13:49.342612Z",
     "start_time": "2020-11-01T06:13:42.677546Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np \n",
    "import time\n",
    "from utils import *\n",
    "\n",
    "from keras.models import Model\n",
    "from numpy import array\n",
    "from keras.models import load_model\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import Utils_SRGAN, Utils_model_SRGAN\n",
    "from Utils_model_SRGAN import VGG_LOSS\n",
    "from scipy.spatial.distance import pdist, squareform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SRGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-01T06:13:49.374603Z",
     "start_time": "2020-11-01T06:13:49.357596Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_VGG_SRGAN():\n",
    "    model_dir = './model-weights/gen_model3000.h5'\n",
    "    image_shape = (96, 96, 3)\n",
    "    \n",
    "    loss = VGG_LOSS(image_shape=image_shape)  \n",
    "    model = load_model(model_dir , custom_objects={'vgg_loss': loss.vgg_loss})\n",
    "    \n",
    "    return model\n",
    "\n",
    "def test_model_for_lr_images(input_low_res, model):\n",
    "\n",
    "    x_test_lr = Utils_SRGAN.LOAD_DATA_TEST(input_low_res)\n",
    "    output_high_res = Utils_SRGAN.plot_test_generated_images(model, x_test_lr)\n",
    "    return output_high_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions used to detect faces in an image\n",
    "**- load_yolo_face** loads the config and weight files for Yolo-v3 trained for Face Detection\n",
    "\n",
    "**- face_detection** detects faces and stores them in 'image_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-01T06:13:49.438616Z",
     "start_time": "2020-11-01T06:13:49.379603Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_yolo_face():\n",
    "    \n",
    "    model_cfg = './cfg/yolov3-face.cfg'\n",
    "    model_weights = './model-weights/yolov3-wider_16000.weights'\n",
    "    \n",
    "    net = cv2.dnn.readNetFromDarknet(model_cfg, model_weights)\n",
    "    net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "    net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n",
    "    \n",
    "    return net\n",
    "\n",
    "def face_detection(net, frame, current_frame, current_person, model_vgg):\n",
    "\n",
    "        # Create a 4D blob from a frame.\n",
    "        blob = cv2.dnn.blobFromImage(frame, 1 / 255, (IMG_WIDTH, IMG_HEIGHT), [0, 0, 0], 1, crop=False)\n",
    "\n",
    "        # Sets the input to the network\n",
    "        net.setInput(blob)\n",
    "\n",
    "        # Runs the forward pass to get output of the output layers\n",
    "        outs = net.forward(get_outputs_names(net))\n",
    "\n",
    "        # Remove the bounding boxes with low confidence\n",
    "        faces = post_process(frame, outs, CONF_THRESHOLD, NMS_THRESHOLD)\n",
    "        print('[i] ==> # detected faces: {}'.format(len(faces)))\n",
    "        print('#' * 60)\n",
    "\n",
    "        # initialize the set of information we'll displaying on the frame\n",
    "        info = [\n",
    "            ('number of faces detected', '{}'.format(len(faces)))\n",
    "        ]\n",
    "        \n",
    "        # to crop out multiple faces in the image\n",
    "        for f in faces:\n",
    "            x, y, w, h = f\n",
    "\n",
    "            sub_face = frame[y+2:y+h-2, x+2:x+w-2]\n",
    "            final_img = test_model_for_lr_images(sub_face, model_vgg)\n",
    "            #Saves image of the current frame in jpg file\n",
    "            name='./image_data/frame'+str(current_frame) + '_p'+str(current_person)+'.jpg'\n",
    "            print('Creating...'+name)\n",
    "            cv2.imwrite(name,final_img)\n",
    "            #To prevent duplicate images\n",
    "            current_person+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions used to detect objects in an image\n",
    "**- load_yolo** is used to load the config and weight files used to detect objects using COCO dataset\n",
    "\n",
    "**- start_webcam** is used to start the webcam using OpenCV\n",
    "\n",
    "**- detect_objects** is used to detect different objects inn an image\n",
    "\n",
    "**- get_box_dimensions** is used bounding box, scores and class of the detected object\n",
    "\n",
    "**- draw_labels** is used to draw labels above the bounding box of the detected object\n",
    "\n",
    "**- webcam_detect** is used to call all the above functions to perform the whole process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-01T06:26:38.259496Z",
     "start_time": "2020-11-01T06:26:37.768378Z"
    }
   },
   "outputs": [],
   "source": [
    "#Load yolo\n",
    "def load_yolo():\n",
    "    net = cv2.dnn.readNet(\"model-weights/yolov3.weights\", \"cfg/yolov3.cfg\")\n",
    "    classes = []\n",
    "    with open(\"cfg/coco.names\", \"r\") as f:\n",
    "        classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "    layers_names = net.getLayerNames()\n",
    "    output_layers = [layers_names[i[0]-1] for i in net.getUnconnectedOutLayers()]\n",
    "    colors = np.random.uniform(0, 255, size=(len(classes), 3))\n",
    "    return net, classes, colors, output_layers\n",
    "\n",
    "\n",
    "def detect_objects(img, net, outputLayers):\n",
    "    blob = cv2.dnn.blobFromImage(img, scalefactor=0.00392, size=(320, 320), mean=(0, 0, 0), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outputs = net.forward(outputLayers)\n",
    "    return blob, outputs\n",
    "\n",
    "def get_box_dimensions(outputs, height, width):\n",
    "    boxes = []\n",
    "    confs = []\n",
    "    class_ids = []\n",
    "    for output in outputs:\n",
    "        for detect in output:\n",
    "            scores = detect[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            conf = scores[class_id]\n",
    "            if conf > 0.3:\n",
    "                center_x = int(detect[0] * width)\n",
    "                center_y = int(detect[1] * height)\n",
    "                w = int(detect[2] * width)\n",
    "                h = int(detect[3] * height)\n",
    "                x = int(center_x - w/2)\n",
    "                y = int(center_y - h / 2)\n",
    "                boxes.append([x, y, w, h])\n",
    "                confs.append(float(conf))\n",
    "                class_ids.append(class_id)\n",
    "    return boxes, confs, class_ids\n",
    "\n",
    "\n",
    "def draw_labels_video(boxes, confs, colors, class_ids, classes, img, current_frame, net, model_vgg, matrix, bird_image, rect_boundary, threshold): \n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confs, 0.5, 0.4)\n",
    "    font = cv2.FONT_HERSHEY_PLAIN\n",
    "    \n",
    "    # drawing ROI on the frame\n",
    "    cv2.line(img,rect_boundary[0],rect_boundary[1],(255,255,0),3)\n",
    "    cv2.line(img,rect_boundary[0],rect_boundary[2],(255,255,0),3)\n",
    "    cv2.line(img,rect_boundary[1],rect_boundary[3],(255,255,0),3)\n",
    "    cv2.line(img,rect_boundary[2],rect_boundary[3],(255,255,0),3)\n",
    "    \n",
    "    current_person = 0\n",
    "    person_pts = []\n",
    "    warped_pts = []\n",
    "    bb_pts = []\n",
    "    violators_warp = []\n",
    "    pers_violators = []\n",
    "    \n",
    "    # extracting bounding box and warped points for 'person' class\n",
    "    for i in range(len(boxes)):\n",
    "        if i in indexes:\n",
    "            x, y, w, h = boxes[i]\n",
    "            label = str(classes[class_ids[i]])\n",
    "            if label == 'person':\n",
    "                \n",
    "                bb_pts.append((x,y,w,h))\n",
    "                \n",
    "                X_mid = x+w//2\n",
    "                mid_bottom_pt = (X_mid, y+h)\n",
    "                person_pts.append(mid_bottom_pt)\n",
    "                \n",
    "                person_array = np.array([[[mid_bottom_pt[0], mid_bottom_pt[1]]]], dtype = 'float32')\n",
    "                warped_coordinate = cv2.perspectiveTransform(person_array, matrix)[0][0]\n",
    "                warped_pts.append((warped_coordinate[0], warped_coordinate[1]))\n",
    "    \n",
    "    \n",
    "    # calculating social distancing violators\n",
    "    for i in range(0, len(warped_pts)-1):\n",
    "        for j in range(i+1, len(warped_pts)):\n",
    "            \n",
    "            x1, y1 = warped_pts[i]\n",
    "            x2, y2 = warped_pts[j]\n",
    "            dist = int(round(np.sqrt((x1 - x2)**2 + (y1 - y2)**2)))\n",
    "            \n",
    "            # if distance between the two points is less than allowed distance, then add them to voilators\n",
    "            if dist < int(threshold):\n",
    "                if warped_pts[i] not in violators_warp:\n",
    "                    violators_warp.append(warped_pts[i])\n",
    "                    pers_violators.append(person_pts[i])\n",
    "                    \n",
    "                if warped_pts[j] not in violators_warp:\n",
    "                    violators_warp.append(warped_pts[j])\n",
    "                    pers_violators.append(person_pts[j])\n",
    "    \n",
    "    \n",
    "    # assigning colours to the violators and non-violators\n",
    "    for i in range(0, len(warped_pts)):\n",
    "        newImage = img.copy()\n",
    "        x, y, w, h = bb_pts[i]\n",
    "        axesLen = (w, h//2)\n",
    "        angle = 115\n",
    "        sub_person = newImage[y+1:y+h-1, x+1:x+w-1]\n",
    "        \n",
    "        if warped_pts[i] in violators_warp:\n",
    "\n",
    "            cv2.rectangle(img, (x,y), (x+w, y+h), (0, 0, 255), 2)\n",
    "            cv2.circle(img, person_pts[i], 5, (0,0, 255), -1)\n",
    "                    \n",
    "            cv2.circle(bird_image, warped_pts[i], 10, (0, 0, 255), -1)\n",
    "            cv2.circle(bird_image, warped_pts[i], int(threshold), (0, 0, 255), 1)\n",
    "            \n",
    "        else:\n",
    "\n",
    "            cv2.rectangle(img, (x,y), (x+w, y+h), (0, 255, 0), 2)\n",
    "            cv2.circle(img, person_pts[i], 5, (0, 255, 0), -1)\n",
    "                    \n",
    "            cv2.circle(bird_image, warped_pts[i], 10, (0, 255, 0), -1)\n",
    "            cv2.circle(bird_image, warped_pts[i], int(threshold), (0, 255, 0), 1)\n",
    "    \n",
    "    #stats\n",
    "    total_people = len(warped_pts)\n",
    "    total_violators = len(violators_warp)\n",
    "    total_non_violators = total_people - total_violators\n",
    "    \n",
    "    # creating headings \n",
    "    org = (5, 35)\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX \n",
    "    fontScale = 1\n",
    "    color = (0, 0, 0)\n",
    "    thickness = 2\n",
    "\n",
    "    heading_normal = np.zeros((50, 1067, 3), np.uint8)\n",
    "    heading_normal[:] = (145, 249, 255)\n",
    "    heading_normal = cv2.putText(heading_normal, 'Normal View', org, font, fontScale, color, thickness, cv2.LINE_AA) \n",
    "    \n",
    "    heading_bird = np.zeros((50, 400, 3), np.uint8)\n",
    "    heading_bird[:] = (105, 186, 255)\n",
    "    heading_bird = cv2.putText(heading_bird, \"Bird's Eye View\", org, font, fontScale, color, thickness, cv2.LINE_AA) \n",
    "\n",
    "    heading = cv2.hconcat([heading_normal, heading_bird])\n",
    "    \n",
    "    \n",
    "    img = cv2.resize(img, (1067, 600))\n",
    "    res = cv2.hconcat([img, bird_image])\n",
    "    \n",
    "    im_v = cv2.vconcat([heading, res])\n",
    "    \n",
    "    # stats \n",
    "    stats = np.zeros((150, 1467, 3), np.uint8)\n",
    "    stats[:] = (255, 255, 255)\n",
    "    stats = cv2.putText(stats, 'Stats (Social Distancing) :-', (10, 55), font, fontScale, color, thickness, cv2.LINE_AA) \n",
    "    total_str = 'Total people : ' + str(total_people)\n",
    "    stats = cv2.putText(stats, total_str, (10, 120), font, fontScale, color, thickness, cv2.LINE_AA) \n",
    "    viol_str = 'Violators : ' + str(total_violators)\n",
    "    stats = cv2.putText(stats, viol_str, (450, 120), font, fontScale, color, thickness, cv2.LINE_AA) \n",
    "    non_viol_str = 'Non-Violators : ' + str(total_non_violators)\n",
    "    stats = cv2.putText(stats, non_viol_str, (890, 120), font, fontScale, color, thickness, cv2.LINE_AA)\n",
    "    \n",
    "    f_res = cv2.vconcat([im_v, stats])\n",
    "    \n",
    "    cv2.imshow(\"Output\", f_res)\n",
    "    \n",
    "    return f_res\n",
    "\n",
    "\n",
    "def get_mouse_points(event, x, y, flags, param):\n",
    "\n",
    "    global mouseX, mouseY, mouse_pts\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        mouseX, mouseY = x, y\n",
    "        cv2.circle(image, (x, y), 10, (0, 255, 255), 10)\n",
    "        if \"mouse_pts\" not in globals():\n",
    "            mouse_pts = []\n",
    "        mouse_pts.append((x, y))\n",
    "        print(\"Lenght of list : \", len(mouse_pts))\n",
    "        print(\"Point detected\")\n",
    "        print(mouse_pts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-01T06:30:47.857099Z",
     "start_time": "2020-11-01T06:26:40.082930Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO-v3 for Person Detection loaded\n",
      "------------------------------------------------\n",
      "YOLO-v3 for Face Detection loaded\n",
      "------------------------------------------------\n",
      "SRGAN loaded\n",
      "------------------------------------------------\n",
      "Lenght of list :  1\n",
      "Point detected\n",
      "[(6, 441)]\n",
      "Lenght of list :  2\n",
      "Point detected\n",
      "[(6, 441), (1049, 703)]\n",
      "Lenght of list :  3\n",
      "Point detected\n",
      "[(6, 441), (1049, 703), (733, 4)]\n",
      "Lenght of list :  4\n",
      "Point detected\n",
      "[(6, 441), (1049, 703), (733, 4), (1220, 20)]\n",
      "Lenght of list :  5\n",
      "Point detected\n",
      "[(6, 441), (1049, 703), (733, 4), (1220, 20), (720, 374)]\n",
      "Lenght of list :  6\n",
      "Point detected\n",
      "[(6, 441), (1049, 703), (733, 4), (1220, 20), (720, 374), (717, 202)]\n",
      "------ ROI defined ------\n",
      "------ THRESHOLD defined ------\n",
      "**------- END OF VIDEO FILE -------**\n"
     ]
    }
   ],
   "source": [
    "#######\n",
    "video_path = \"test_video.mp4\"\n",
    "\n",
    "model, classes, colors, output_layers = load_yolo()\n",
    "print(\"YOLO-v3 for Person Detection loaded\")\n",
    "print(\"------------------------------------------------\")\n",
    "net = load_yolo_face()\n",
    "print(\"YOLO-v3 for Face Detection loaded\")\n",
    "print(\"------------------------------------------------\")\n",
    "    \n",
    "\"\"\"\n",
    "K.set_image_data_format('channels_first')\n",
    "FRmodel = load_face_recog_model()\n",
    "print(\"Face Recognition Model loaded\")\n",
    "print(\"------------------------------------------------\")\n",
    "database = load_database(FRmodel)\n",
    "print(\"User Database loaded\")\n",
    "print(\"------------------------------------------------\")\n",
    "K.set_image_data_format('channels_last')\n",
    "\"\"\"\n",
    "    \n",
    "model_vgg = load_VGG_SRGAN()\n",
    "print(\"SRGAN loaded\")\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "current_frame = 0\n",
    "\n",
    "sz = (1467, 800)\n",
    "out1 = cv2.VideoWriter(\"output/output_social_distancing.avi\",cv2.VideoWriter_fourcc(*'XVID'), int(fps), sz)\n",
    "\n",
    "f_no = 1\n",
    "    \n",
    "    \n",
    "    \n",
    "while cap.isOpened():\n",
    "        \n",
    "        \n",
    "        \n",
    "    ret, frame = cap.read()\n",
    "        \n",
    "    bird_image = np.zeros((600, 400, 3), np.uint8)\n",
    "    bird_image[:] = (41, 41, 41)\n",
    "        \n",
    "    if ret == True:\n",
    "        \n",
    "        if f_no == 1:\n",
    "                \n",
    "            mouse_pts = []    \n",
    "            cv2.namedWindow(\"image\")\n",
    "            cv2.setMouseCallback(\"image\", get_mouse_points)\n",
    "            num_mouse_points = 0\n",
    "            \n",
    "            while True:\n",
    "                image = frame\n",
    "                cv2.imshow(\"image\", image)\n",
    "                cv2.waitKey(1)\n",
    "                    \n",
    "                if len(mouse_pts) == 6:\n",
    "                    f_no = f_no + 1\n",
    "                    cv2.destroyWindow(\"image\")\n",
    "                    break\n",
    "                four_points = mouse_pts\n",
    "                \n",
    "            pt1 = four_points[0:4]\n",
    "            print(\"------ ROI defined ------\")\n",
    "            pt2 = [(0, 600), (400, 600), (0, 0), (400, 0)]\n",
    "            \n",
    "            pts1 = np.float32(np.array(pt1)) \n",
    "            pts2 = np.float32(np.array(pt2)) \n",
    "            \n",
    "            matrix = cv2.getPerspectiveTransform(pts1, pts2) \n",
    "            \n",
    "            ## threshold calculation\n",
    "            t_pt1 = four_points[4]\n",
    "            t_pt2 = four_points[5]\n",
    "            \n",
    "            thresh_pt1 = np.array([[[t_pt1[0], t_pt1[1]]]], dtype = 'float32')\n",
    "            warped_t1 = cv2.perspectiveTransform(thresh_pt1, matrix)[0][0]\n",
    "            \n",
    "            thresh_pt2 = np.array([[[t_pt2[0], t_pt2[1]]]], dtype = 'float32')\n",
    "            warped_t2 = cv2.perspectiveTransform(thresh_pt2, matrix)[0][0]\n",
    "            \n",
    "            threshold = round(np.sqrt((warped_t1[0] - warped_t2[0])**2 + (warped_t1[1] - warped_t2[1])**2))\n",
    "            print(\"------ THRESHOLD defined ------\")\n",
    "        \n",
    "        height, width, channels = frame.shape\n",
    "        blob, outputs = detect_objects(frame, model, output_layers)\n",
    "        boxes, confs, class_ids = get_box_dimensions(outputs, height, width)\n",
    "        image = draw_labels_video(boxes, confs, colors, class_ids, classes, frame, current_frame, net, model_vgg, matrix, bird_image, pt1, threshold)\n",
    "        current_frame += 1        \n",
    "\n",
    "        out1.write(image)\n",
    "        #out2.write(bird_image)\n",
    "        \n",
    "        key = cv2.waitKey(9)\n",
    "        if key == 27 or key == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        print(\"**------- END OF VIDEO FILE -------**\")\n",
    "        break\n",
    "            \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
